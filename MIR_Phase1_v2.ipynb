{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-b2UsRU4P9JF"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>فاز اول پروژه</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل: ۲۸ اسفند ۱۳۹۹\n",
    "<br>\n",
    "<br>\n",
    "<font size=4.8>\n",
    "دستیاران آموزشی: نیما جمالی، آرمین سعادت، ایمان غلامی\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3YoNVsQR3zO"
   },
   "source": [
    "<p></p>\n",
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>مقدمه</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "    هدف از فاز اول پروژه، طراحی و پیاده‌سازی سیستم بازیابی اطلاعات برای مجموعه دادگان تعیین شده می‌باشد.<br>\n",
    "    اولین مرحله، پیش‌پردازش مجموعه دادگان است. پس از آن نمایه‌ها با ویژگی‌های خواسته شده پیاده‌سازی می‌شود. در گام بعدی به ذخیره و بازخوانی نمایه‌ها به همراه روش‌های فشرده‌سازی پرداخته می‌شود. پس از آن تکنیک‌های اصلاح پرسمان پیاده‌سازی شده و در نهایت جستجو روی دادگان صورت می‌گیرد. همچنین در بخش آخر با پیاده‌سازی برخی معیارهای ارزیابی، عملکرد سیستم مورد سنجش قرار می‌گیرد.<br><br>\n",
    "     توضیحات مربوط به هر بخش در ادامه آمده است که اهداف، محدودیت‌ها و خواسته‌های آن بخش را مشخص می‌کند.\n",
    "     در هر بخش توابعی مشخص شده است که محتوای آن با کد نوشته شده توسط شما باید پر شود. شما می‌توانید در همین فایل، سیستم بازیابی خود را پیاده‌سازی کنید یا فایل‌های خودتان را ایمپورت کرده و از آن‌ها استفاده کنید. در هر صورت تمامی کدهای لازم را به همراه این نوت‌بوک ارسال کنید. همچنین در نظر داشته باشید که ملاک اصلی نمره‌دهی شما اجرای صحیح توابع مشخص‌شده در این نوت‌بوک می‌باشد. بنابراین از صحت اجرای نوت‌بوک خود اطمینان پیدا کنید.<br><br>\n",
    "     تنها زبان قابل قبول برای پروژه پایتون است. محدودیت استفاده از کتاب‌خانه‌های آماده در هر بخش مشخص شده است. پروژه ۱۱۵ نمره دارد که ۱۵ نمره از آن امتیازی و مربوط به بخش اصلاح پرسمان می‌باشد.\n",
    "    \n",
    "    \n",
    "</font>\n",
    "</div>\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaR5CS_khMQB"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>مجموعه دادگان</b>\n",
    "    </h1>\n",
    "    مجموعه دادگان مورد بررسی در این پروژه از سایت kaggle فراهم شده است. این مجموعه شامل اطلاعات ۶۰۰۰ فیلم سینمایی از سال ۱۹۰۴ تا ۲۰۱۷ است. داده‌ها در قالب فایل csv   دارای ستون‌‌های plot، title، id می‌باشد. id یک شناسه یکتا برای هر فیلم است که برای ارزیابی بهتر عملکرد شما به داده‌ها اضافه شده است و در مجموعه دادگان اصلی وجود نداشته است. همانطور که می‌دانید برای پیاده‌سازی نمایه باید به هر داکیومنت یک شناسه اختصاص بدهید. <b>شناسه مربوط به هر داکیومنت باید id ذکر شده برای آن در مجموعه دادگان باشد.</b> هر فیلم از دو بخش title و plot  تشکیل شده است که از این دو بخش در ساخت نمایه و جستجو استفاده می‌شود. plot خلاصه‌ای از طرح داستان فیلم است.<br>\n",
    "    علاوه بر مجموعه دادگان اصلی، تعدای پرسمان در اختیار شما قرار گرفته است. همچنین جواب مطلوب هر یک از این پرسمان‌ها نیز فراهم شده که طبیعتا زیرمجموعه‌ای از مجموعه دادگان است. شما باید از این پرسمان‌ها و نتیجه مورد انتظار هر کدام برای ارزیابی سامانه خود استفاده کنید.\n",
    "    پرسمان‌ها و شناسه فیلم‌های بازیابی شده مورد انتظار از هر پرسمان در فایل validation.json آمده است. توضیحات بیشتر در رابطه با استفاده از این فایل در بخش ارزیابی آمده است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxNntHofmHHH"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پیش‌پردازش و آماده‌سازی داده‌ها (۱۵ نمره)</b>\n",
    "    </h1>\n",
    "    در این بخش ابتدا داده‌ها را از فایل بخوانید. برای آماده‌سازی متن می‌توانید از کتاب‌خانه‌های آماده استفاده کنید. یکی از کتاب‌خانه‌های معروف برای این کار <a href=\"https://www.nltk.org/\">NLTK</a> است اما در انتخاب روش پیاده‌سازی این بخش مختارید. برای این بخش باید تابع ()prepare_text را تکمیل کنید. این تابع یک متن انگلیسی ورودی گرفته و توکن‌‌های مربوط به آن‌را در قالب یک لیست خروجی می‌دهد. متن ورودی در عمل تایتل یا طرح داستان هر فیلم است. دقت کنید که لیست خروجی شامل تعدادی توکن است که عملیات case folding، stemming و lemmatization روی آن‌ها اجرا شده است. در ضمن علائم نگارشی نباید به عنوان توکن در نظر گرفته شود. در کد زیر یک نمونه ورودی و خروجی نمایش داده شده است. با توجه به نحوه پیاده‌سازی انواع بازگردانی به ریشه قابل قبول است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9aa16Puk-if0",
    "outputId": "2e6a2c95-41d5-4146-a27b-5ceba034e291"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "def prepare_text(raw_text):\n",
    "    trantab = str.maketrans(dict.fromkeys(list(string.punctuation)))\n",
    "    raw_text = raw_text.translate(trantab)\n",
    "\n",
    "    tokens = [word.lower() for word in word_tokenize(raw_text)]\n",
    "\n",
    "    stemmer = PorterStemmer(PorterStemmer.ORIGINAL_ALGORITHM)\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(token, pos='a') for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['edvard', 'wa', 'a', 'runner', 'he', 'wa', 'alwai', 'run']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_text(\"Edvard was a Runner. He was always running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVhVEO6VARIa"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>شناسایی و حذف stop-words (۵ نمره)</b>\n",
    "    </h1>\n",
    "    این بخش باید توسط خودتان و بدون استفاده از کد آماده پیاده‌سازی شود. ترم‌های موجود در مجموعه دادگان را بر اساس تکرار آن‌ها مرتب کرده و پرتکرارترین آن‌ها را به عنوان stop-words در نظر بگیرید. اینکه چند ترم را به عنوان stop-words در نظر بگیرید به عهده خودتان است.<br>\n",
    "    با فراخوانی تابع ()get_stop_words لیست stop-words به همراه تعداد تکرار آن‌‌ها خروجی داده می‌شود.<br>\n",
    "    ترم‌های به دست آمده از این بخش نباید در نمایه حضور داشته باشند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-0YxOUeBGDY",
    "outputId": "b8e3476a-02af-41dc-b4a3-5c637ffd4118"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def get_stop_words():\n",
    "    tokens = []\n",
    "\n",
    "    with open('movies.csv', encoding=\"utf8\") as file:\n",
    "        data = file.read()\n",
    "\n",
    "        trantab = str.maketrans(dict.fromkeys(list(string.punctuation)))\n",
    "        data = data.translate(trantab)\n",
    "\n",
    "        tokens = tokens + [word.lower() for word in word_tokenize(data)]\n",
    "\n",
    "    count_dict = {}\n",
    "    result = {}\n",
    "\n",
    "    for token in tokens:\n",
    "        if count_dict.__contains__(token):\n",
    "            count_dict[token] += 1\n",
    "        else:\n",
    "            count_dict[token] = 1\n",
    "\n",
    "    for i in range(45):\n",
    "        max_key = max(count_dict, key=count_dict.get)\n",
    "        result[max_key] = count_dict[max_key]\n",
    "        count_dict.pop(max_key)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "stop_words = get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 145374,\n",
       " 'to': 91548,\n",
       " 'and': 83076,\n",
       " 'a': 67203,\n",
       " 'of': 42762,\n",
       " 'is': 40218,\n",
       " 'in': 37744,\n",
       " 'his': 34696,\n",
       " 'he': 31674,\n",
       " 'her': 27914,\n",
       " 'that': 26221,\n",
       " 'with': 25040,\n",
       " 'him': 18898,\n",
       " 'by': 17809,\n",
       " 'for': 17567,\n",
       " 'she': 17217,\n",
       " 'as': 15538,\n",
       " 'on': 14722,\n",
       " 'but': 13502,\n",
       " 'they': 13134,\n",
       " 'who': 12616,\n",
       " 'at': 12278,\n",
       " 'from': 11090,\n",
       " 'has': 10949,\n",
       " 'an': 10788,\n",
       " 'when': 9886,\n",
       " 'their': 9474,\n",
       " 'are': 9220,\n",
       " 'after': 8718,\n",
       " 'it': 8690,\n",
       " 'out': 7798,\n",
       " 'into': 7320,\n",
       " 'up': 6760,\n",
       " 'be': 6570,\n",
       " 'them': 6542,\n",
       " 'not': 5884,\n",
       " 'was': 5302,\n",
       " 'while': 5187,\n",
       " 'one': 5152,\n",
       " 'then': 4967,\n",
       " 'which': 4589,\n",
       " 'about': 4482,\n",
       " 'will': 4461,\n",
       " 'where': 4396,\n",
       " 'have': 4322}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4Z02BzNHD1z"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>نمایه‌سازی (۱۵ نمره)</b>\n",
    "    </h1>\n",
    "    در این بخش باید برای سامانه positional index بسازید. برای هر ترم باید مشخص باشد که آن ترم در تایتل چه فیلم‌هایی و در چه جایگاهی از تایتل هر فیلم قرار گرفته است. همچنین برای هر ترم باید مشخص باشد که آن ترم در طرح داستان چه فیلم‌هایی و در چه جایگاهی از طرح داستان هر فیلم قرار گرفته است.<br>\n",
    "     برای ارزیابی بهتر و عادلانه‌تر به ویژه برای ارزیابی بخش فشرده‌سازی، از استاندارد بیان شده در قطعه کد زیر برای ذخیره posting list هر ترم در RAM استفاده کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJy4LiraBm8E",
    "outputId": "a740d94d-1896-4a6a-9a28-68b1cb8d32db"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "title_tokens = dict()\n",
    "plot_tokens = dict()\n",
    "titles = dict()\n",
    "plots = dict()\n",
    "\n",
    "\n",
    "def create_positional_index():\n",
    "    positional_index = dict()\n",
    "\n",
    "    with open('movies.csv', encoding=\"utf8\") as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        first_line = True\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if first_line:\n",
    "                first_line = False\n",
    "                continue\n",
    "\n",
    "            doc_id = int(row[0])\n",
    "\n",
    "            title = row[1]\n",
    "            plot = row[2]\n",
    "\n",
    "            titles[doc_id] = title\n",
    "            plots[doc_id] = plots\n",
    "\n",
    "            add_positions(positional_index, title, doc_id, 0)\n",
    "            add_positions(positional_index, plot, doc_id, 1)\n",
    "\n",
    "    return positional_index\n",
    "\n",
    "\n",
    "def add_positions(positional_index, raw_text, doc_id, column):\n",
    "    tokens = prepare_text(raw_text)\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "\n",
    "    if column == 0:\n",
    "        title_tokens[doc_id] = tokens\n",
    "    else:\n",
    "        plot_tokens[doc_id] = tokens\n",
    "\n",
    "    if column == 0:\n",
    "        titles[doc_id] = raw_text\n",
    "    else:\n",
    "        plots[doc_id] = raw_text\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for pos, term in enumerate(tokens):\n",
    "        if term not in result:\n",
    "            result[term] = list()\n",
    "\n",
    "        result[term].append(pos)\n",
    "\n",
    "    for term in result.keys():\n",
    "        if term not in positional_index:\n",
    "            positional_index[term] = [list(), list()]\n",
    "\n",
    "        positional_index[term][column].append([doc_id, result[term]])\n",
    "\n",
    "\n",
    "positional_index = create_positional_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voqMrk4rg1qk"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پویا‌سازی نمایه (۱۰ نمره)</b>\n",
    "    </h1>\n",
    "    نمایه ایجاد شده باید قابلیت حذف و اضافه تک داکیومنت را داشته باشد.\n",
    "    برای اضافه شدن داکیومنت، به تابع ()add_single_document یک رشته داده می‌شود که اطلاعات مربوط به داکیومنت شامل id و plot و title در آن با کاما جدا شده است. برای حذف داکیومنت نیز id آن به تابع ()remove_single_document داده می‌شود.<br>\n",
    "    تضمین می‌شود که شرط یکتا بودن id داکیومنت‌ها نقض نشود. برای مثال دو داکیومنت با شناسه یکسان به مجموعه اضافه نخواهد شد. البته ممکن است حذف شده و دوباره اضافه شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e1Ej2n3MB6ef"
   },
   "outputs": [],
   "source": [
    "def add_single_documnet(document):\n",
    "    f = open(\"temp.csv\", \"w\")\n",
    "    f.write(document)\n",
    "    f.close()\n",
    "\n",
    "    with open('temp.csv', encoding=\"utf8\") as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        for row in csv_reader:\n",
    "            id = int(row[0])\n",
    "            title = row[1]\n",
    "            plot = row[2]\n",
    "\n",
    "            add_positions(positional_index, title, id, 0)\n",
    "            add_positions(positional_index, plot, id, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_document = \"1234,The Adventures of Sherlock Holmes,The picture begins with Moriarty and Holmes verbally sparring \" \\\n",
    "               \"on the steps \"\n",
    "add_single_documnet(new_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DsarzVzhDoMi"
   },
   "outputs": [],
   "source": [
    "def remove_signle_document(doc_id):\n",
    "    del title_tokens[doc_id]\n",
    "    del plot_tokens[doc_id]\n",
    "    del titles[doc_id]\n",
    "    del plots[doc_id]\n",
    "\n",
    "    for term in positional_index:\n",
    "        for i in range(0, 2):\n",
    "            posting_list = positional_index[term][i]\n",
    "\n",
    "            for i in range(len(posting_list) - 1, -1, -1):\n",
    "                if posting_list[i][0] == doc_id:\n",
    "                    del posting_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_signle_document(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PBbFcmpXFKD"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>ذخیره و فشرده‌سازی نمایه (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "    در این بخش باید توانایی ذخیره کردن نمایه و بارگذاری مجدد آن را به سامانه اضافه کنید. ذخیره‌سازی به ۳ روش صورت می‌گیرد. بدون فشرده‌سازی، فشرده‌سازی از روش gamma-code و فشرده‌سازی از روش variable-byte. روش‌های فشرده‌سازی باید توسط خودتان پیاده‌سازی شود.\n",
    "    برای ذخیره نمایه در فایل نیز از JSON  استفاده کنید.<br>\n",
    "     بخشی از نمره شما در این قسمت به میزان فشرده‌سازی نمایه اختصاص داده شده است. بنابراین پیاده‌سازی بهینه روش‌های فشرده‌سازی مهم است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from struct import pack, unpack\n",
    "\n",
    "\n",
    "def encode_number(number):\n",
    "    bytes_list = []\n",
    "    while True:\n",
    "        bytes_list.insert(0, number % 128)\n",
    "        if number < 128:\n",
    "            break\n",
    "        number = number // 128\n",
    "    bytes_list[-1] += 128\n",
    "    return pack('%dB' % len(bytes_list), *bytes_list)\n",
    "\n",
    "\n",
    "def variable_byte_encode(numbers):\n",
    "    bytes_list = []\n",
    "    for number in numbers:\n",
    "        bytes_list.append(encode_number(number))\n",
    "    return b\"\".join(bytes_list)\n",
    "\n",
    "\n",
    "def variable_byte_decode(bytestream):\n",
    "    n = 0\n",
    "    numbers = []\n",
    "    bytestream = unpack('%dB' % len(bytestream), bytestream)\n",
    "    for byte in bytestream:\n",
    "        if byte < 128:\n",
    "            n = 128 * n + byte\n",
    "        else:\n",
    "            n = 128 * n + (byte - 128)\n",
    "            numbers.append(n)\n",
    "            n = 0\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "log2 = lambda x: log(x, 2)\n",
    "\n",
    "\n",
    "def binary(x, l):\n",
    "    fmt = '{0:0%db}' % l\n",
    "    return fmt.format(x)\n",
    "\n",
    "\n",
    "def unary(x):\n",
    "    return x * '1' + '0'\n",
    "\n",
    "\n",
    "def elias_generic(lencoding, x):\n",
    "    if x == 0:\n",
    "        return '0'\n",
    "\n",
    "    l = 1 + int(log2(x))\n",
    "    a = x - 2 ** (int(log2(x)))\n",
    "\n",
    "    k = int(log2(x))\n",
    "\n",
    "    return lencoding(l) + binary(a, k)\n",
    "\n",
    "\n",
    "def gamma_code_encode(x):\n",
    "    return hex(int(elias_generic(unary, x), 2))[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYN9I4_BD178",
    "outputId": "4a11351f-d214-495e-eb3f-7dc56863cd58"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "\n",
    "\n",
    "def store_index(path, compression_type):\n",
    "    if compression_type == \"no-compression\":\n",
    "        no_compression_store_index(path)\n",
    "    else:\n",
    "        compression_store_index(path, compression_type)\n",
    "\n",
    "    return os.path.getsize(path)\n",
    "\n",
    "\n",
    "def no_compression_store_index(path):\n",
    "    with open(path, \"w\") as outfile:\n",
    "        json.dump(positional_index, outfile)\n",
    "\n",
    "\n",
    "def compression_store_index(path, compression_type):\n",
    "    copy_positional_index = copy.deepcopy(positional_index)\n",
    "\n",
    "    for term in copy_positional_index:\n",
    "        for i in range(0, 2):\n",
    "            posting_list = copy_positional_index[term][i]\n",
    "            previous = None\n",
    "\n",
    "            for doc in posting_list:\n",
    "                if previous is None:\n",
    "                    previous = doc[0]\n",
    "                else:\n",
    "                    previous, doc[0] = doc[0], doc[0] - previous\n",
    "\n",
    "                if compression_type == \"variable-byte\":\n",
    "                    doc[0] = variable_byte_encode([doc[0]]).hex()\n",
    "\n",
    "                if compression_type == \"gamma-code\":\n",
    "                    doc[0] = gamma_code_encode(doc[0])\n",
    "\n",
    "                previous2 = None\n",
    "                for j in range(0, len(doc[1])):\n",
    "                    if previous2 is None:\n",
    "                        previous2 = doc[1][j]\n",
    "                    else:\n",
    "                        previous2, doc[1][j] = doc[1][j], doc[1][j] - previous2\n",
    "\n",
    "                if compression_type == \"variable-byte\":\n",
    "                    doc[1] = variable_byte_encode(doc[1]).hex()\n",
    "                if compression_type == \"gamma-code\":\n",
    "                    doc[1] = [gamma_code_encode(x) for x in doc[1]]\n",
    "\n",
    "    with open(path, \"w\") as outfile:\n",
    "        json_str = json.dumps(copy_positional_index)\n",
    "        json_str = json_str.replace(\" \", \"\")\n",
    "        outfile.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17736396\n",
      "15140072\n",
      "20297452\n"
     ]
    }
   ],
   "source": [
    "print(store_index(\"file_nc.json\", \"no-compression\"))\n",
    "print(store_index(\"file_vb.json\", \"variable-byte\"))\n",
    "print(store_index(\"file_vb.json\", \"gamma-code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3Dm38ZXHFQVH"
   },
   "outputs": [],
   "source": [
    "def load_index(path, compression_type):\n",
    "    if compression_type == \"no-compression\":\n",
    "        return no_compression_load_index(path)\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def no_compression_load_index(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_index = load_index(\"file_nc.json\", \"no-compression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i-iLvu2nh2k"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>اصلاح پرسمان (۱۵ نمره امتیازی)</b>\n",
    "    </h1>\n",
    "    در صورتی که پرسمان ورودی دارای غلط املایی باشد یا به عبارتی لغاتی از آن در لغت‌نامه موجود نباشد، لازم است که با جستجوی لغت‌های احتمالی و انتخاب بهترین لغت به ادامه‌ی جستجو با پرسمان اصلاح شده پرداخته شود. برای اینکار ابتدا باید با روش bigram و معیار jaccard نزدیک‌ترین لغات به لغت با غلط املایی را پیدا کنید. سپس با استفاده از معیار edit distance بهترین لغت را از میان آن‌ها بیابید.<br>\n",
    "    نیازی به ذخیره‌سازی و فشرده‌سازی نمایه بایگرم نیست. همچنین می‌توانید از کد آماده برای محاسبه edit distance استفاده کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qMZTstsbL1G3"
   },
   "outputs": [],
   "source": [
    "from nltk import edit_distance\n",
    "\n",
    "\n",
    "def add_bigram_index(term, index):\n",
    "    new_term = \"$\" + term + \"$\"\n",
    "\n",
    "    for i in range(1, len(new_term)):\n",
    "        word = new_term[i - 1: i + 1]\n",
    "\n",
    "        if not index.__contains__(word):\n",
    "            index[word] = []\n",
    "\n",
    "        index[word].append(term)\n",
    "\n",
    "\n",
    "def create_bigram_index():\n",
    "    index = dict()\n",
    "\n",
    "    for term in positional_index.keys():\n",
    "        add_bigram_index(term, index)\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "bigram_index = create_bigram_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sL3E8C-nL1yG",
    "outputId": "d4da236e-728c-4419-e80e-e8a36bd4e85d"
   },
   "outputs": [],
   "source": [
    "def get_corrected_word(main_raw_word):\n",
    "    raw_word = \"$\" + main_raw_word + \"$\"\n",
    "    word_count = {}\n",
    "\n",
    "    for i in range(1, len(raw_word)):\n",
    "        term = raw_word[i - 1: i + 1]\n",
    "\n",
    "        if bigram_index.__contains__(term):\n",
    "            for word in bigram_index[term]:\n",
    "                if not word_count.__contains__(word):\n",
    "                    word_count[word] = 0\n",
    "                word_count[word] = word_count[word] + 1\n",
    "\n",
    "    n = len(main_raw_word) + 1\n",
    "\n",
    "    candidates = list()\n",
    "\n",
    "    for word in word_count:\n",
    "        if word_count[word] / n >= 1 / 2:\n",
    "            candidates.append(word)\n",
    "\n",
    "    for ed in range(1, 3):\n",
    "        for candidate in candidates:\n",
    "            if edit_distance(main_raw_word, candidate) == ed:\n",
    "                return candidate\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_corrected_text(raw_text):\n",
    "    tokens = prepare_text(raw_text)\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "\n",
    "    wrongs = [token for token in tokens if not positional_index.__contains__(token)]\n",
    "\n",
    "    if len(wrongs) == 0:\n",
    "        return raw_text\n",
    "\n",
    "    corrected_words = dict()\n",
    "\n",
    "    for wrong in wrongs:\n",
    "        corrected_words[wrong] = get_corrected_word(wrong)\n",
    "\n",
    "    corrected_text = \"\"\n",
    "\n",
    "    for main_word in raw_text.split():\n",
    "        word = prepare_text(main_word)[0]\n",
    "\n",
    "        if corrected_words.__contains__(word) and corrected_words[word] is not None:\n",
    "            corrected_text += corrected_words[word] + \" \"\n",
    "        else:\n",
    "            corrected_text += main_word + \" \"\n",
    "\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the adventur of hemlock holmes \n"
     ]
    }
   ],
   "source": [
    "print(get_corrected_text(\"the adevntures of herlock holmes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BWhdHJbs1zy"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>جستجو و بازیابی اسناد (۲۵ نمره)</b>\n",
    "    </h1>\n",
    "    در این بخش جستجو ترتیب‌دار در فضای برداری tf-idf به روش <b>lnn-ltn</b> انجام می‌شود. یک پرسمان title و یک پرسمان plot ورودی گرفته شده و هر کدام در بخش مربوطه از اسناد جستجو می‌شوند. امتیاز نهایی هر سند برابر با جمع وزن‌دار امتیاز به دست آمده از جستجو در بخش title و plot است. به این صورت که وزن plot واحد در نظر گرفته شده و وزن تایتل به عنوان ورودی داده می‌شود.<br>\n",
    "    در نهایت اسناد برتر را نمایش دهید. تعداد حداکثر اسناد برتر نیز به عنوان ورودی داده می‌شود.<br><br>\n",
    "    نکته بسیار مهم نحوه نمایش اسناد انتخابی است. برای نمایش هر سند علاوه بر استفاده از شناسه و تیتر، یک هایلایت برای آن درست کنید. به این معنا که کلمات موجود در پرسمان را که باعث انتخاب سند شده‌اند به همراه ۲-۳ ترم قبل و بعد از آن به عنوان هایلایت آن سند نمایش دهید. اینگونه کاربر می‌تواند خیلی سریع دلیل بازیابی اسناد توسط سامانه را متوجه شود. مشابه کاری که سرچ گوگل انجام می‌دهد و ۲-۳ خط مربوطه را زیر وبسایت‌های پیشنهادی نمایش می‌دهد. طبیعتا راه حل بهینه برای این‌کار استفاده از قابلیت‌های نمایه جایگاهی می‌باشد.<br>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0R2i_O4Gev_",
    "outputId": "7f57be29-eec0-489e-dd61-74d67c923106"
   },
   "outputs": [],
   "source": [
    "from math import log10\n",
    "\n",
    "\n",
    "def search(title_query, plot_query, title_weight, max_result_count=10):\n",
    "    title_query = get_corrected_text(title_query)\n",
    "    plot_query = get_corrected_text(plot_query)\n",
    "\n",
    "    title_query_tokens = prepare_text(title_query)\n",
    "    title_query_tokens = [word for word in title_query_tokens if not word in stop_words]\n",
    "\n",
    "    plot_query_tokens = prepare_text(plot_query)\n",
    "    plot_query_tokens = [word for word in plot_query_tokens if not word in stop_words]\n",
    "\n",
    "    title_ids = search_ids(title_query_tokens, 0)\n",
    "    plot_ids = search_ids(plot_query_tokens, 1)\n",
    "\n",
    "    ids = [x for x in title_ids if plot_ids.__contains__(x)]\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for doc_id in ids:\n",
    "        scores.append([doc_id, score(doc_id, title_query_tokens, plot_query_tokens, title_weight)])\n",
    "\n",
    "    scores.sort(key=get, reverse=True)\n",
    "\n",
    "    while len(scores) > max_result_count:\n",
    "        del scores[max_result_count]\n",
    "\n",
    "    docs = [get_doc(s[0], title_query_tokens, plot_query_tokens) for s in scores]\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "def get_doc(doc_id, title_query_tokens, plot_query_tokens):\n",
    "    title = titles[doc_id]\n",
    "\n",
    "    title_highlight = get_highlight(doc_id, title_query_tokens, 0)\n",
    "    plot_highlight = get_highlight(doc_id, plot_query_tokens, 1)\n",
    "\n",
    "    separator = \" ...  \"\n",
    "\n",
    "    return [doc_id, title, separator.join(title_highlight), separator.join(plot_highlight)]\n",
    "\n",
    "\n",
    "def get_highlight(doc_id, query_tokens, field):\n",
    "    position = []\n",
    "\n",
    "    for query_token in query_tokens:\n",
    "        for doc_pos in positional_index[query_token][field]:\n",
    "            if doc_pos[0] == doc_id:\n",
    "                position += doc_pos[1]\n",
    "\n",
    "    highlights = []\n",
    "    str = \"\"\n",
    "\n",
    "    tokens = title_tokens if field == 0 else plot_tokens\n",
    "\n",
    "    for i in range(0, len(tokens[doc_id])):\n",
    "        isHighlight = -1\n",
    "\n",
    "        for pos in position:\n",
    "            if i == pos:\n",
    "                isHighlight = 1\n",
    "                break\n",
    "            elif pos + 4 > i > pos - 4:\n",
    "                isHighlight = 0\n",
    "\n",
    "        if isHighlight == -1 and str != \"\":\n",
    "            highlights.append(str)\n",
    "            str = \"\"\n",
    "\n",
    "        if isHighlight == 0:\n",
    "            str += tokens[doc_id][i] + \" \"\n",
    "\n",
    "        if isHighlight == 1:\n",
    "            str += \"<b>\" + tokens[doc_id][i] + \"</b> \"\n",
    "\n",
    "    if str != \"\":\n",
    "        highlights.append(str)\n",
    "\n",
    "    return highlights\n",
    "\n",
    "\n",
    "def get(s):\n",
    "    return s[1]\n",
    "\n",
    "\n",
    "def search_ids(query_tokens, field):\n",
    "    if len(query_tokens) == 0:\n",
    "        return title_tokens.keys()\n",
    "\n",
    "    ids = set()\n",
    "\n",
    "    for token in query_tokens:\n",
    "        for position in positional_index[token][field]:\n",
    "            ids.add(position[0])\n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "def score(doc_id, title_query_tokens, plot_query_tokens, title_weight):\n",
    "    result = 0\n",
    "\n",
    "    if len(title_query_tokens) != 0:\n",
    "        result += score_field(doc_id, title_query_tokens, 0) * title_weight\n",
    "\n",
    "    if len(plot_query_tokens) != 0:\n",
    "        result += score_field(doc_id, plot_query_tokens, 1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def score_field(doc_id, querys, field):\n",
    "    tokens = title_tokens[doc_id] if field == 0 else plot_tokens[doc_id]\n",
    "    query_tf_raw = create_tf_raw(querys, tokens)\n",
    "    query_tf_wt = create_tf_wt(query_tf_raw)\n",
    "    query_wt = create_wt(query_tf_wt, field)\n",
    "\n",
    "    doc_tf_raw = create_tf_raw(tokens, querys)\n",
    "    doc_tf_wt = create_tf_wt(doc_tf_raw)\n",
    "\n",
    "    result = 0\n",
    "\n",
    "    for word in query_wt:\n",
    "        result += query_wt[word] * doc_tf_wt[word]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_tf_raw(main, temp):\n",
    "    tf_raw = dict()\n",
    "\n",
    "    for word in main:\n",
    "        if not tf_raw.__contains__(word):\n",
    "            tf_raw[word] = 0\n",
    "\n",
    "        tf_raw[word] = tf_raw[word] + 1\n",
    "\n",
    "    for word in temp:\n",
    "        if not tf_raw.__contains__(word):\n",
    "            tf_raw[word] = 0\n",
    "\n",
    "    return tf_raw\n",
    "\n",
    "\n",
    "def create_tf_wt(tf_raw):\n",
    "    tf_wt = dict()\n",
    "\n",
    "    for key in tf_raw:\n",
    "        value = tf_raw[key]\n",
    "\n",
    "        if value == 0:\n",
    "            tf_wt[key] = 0\n",
    "        else:\n",
    "            tf_wt[key] = 1 + log10(value)\n",
    "\n",
    "    return tf_wt\n",
    "\n",
    "\n",
    "def create_wt(tf_wt, field):\n",
    "    wt = dict()\n",
    "    N = len(title_tokens)\n",
    "\n",
    "    for word in tf_wt:\n",
    "        n = 1\n",
    "\n",
    "        if positional_index.__contains__(word) and len(positional_index[word][field]) > 0:\n",
    "            n = len(positional_index[word][field])\n",
    "\n",
    "        wt[word] = log10(N / n) * tf_wt[word]\n",
    "\n",
    "    return wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5874,\n",
       "  'Interstellar',\n",
       "  '',\n",
       "  'human surviv joseph <b>cooper</b> widow engin former <b>nasa</b> pilot run farm  ...  live posttruth societi <b>cooper</b> i reprimand tell  ...  these lead secret <b>nasa</b> facil head <b>cooper</b> former supervisor professor  ...  black hole name <b>gargantua</b> volunt previous travel  ...  wormhol evalu planet <b>miller</b> edmund mann report  ...  colon habit planet <b>cooper</b> i recruit pilot  ...  return crew consist <b>cooper</b> robot tar case  ...  doyl travers wormhol <b>cooper</b> doyl brand us lander investig <b>miller</b> planet time i  ...  find onli wreckag <b>miller</b> expedit gigant tidal  ...  other two planet <b>cooper</b> rule thei go  ...  messag earth murphi <b>cooper</b> i now scientist  ...  mann attempt kill <b>cooper</b> reveal falsifi data hope rescu steal <b>cooper</b> lander head endur  ...  romilli brand rescu <b>cooper</b> other lander thei  ...  difficult dock maneuv <b>cooper</b> regain control insuffici  ...  resort slingshot around <b>gargantua</b> cost anoth 51 year process <b>cooper</b> tar must jettison  ...  past event horizon <b>gargantua</b> thei eject craft  ...  futur across time <b>cooper</b> can see through  ...  complet collaps eject <b>cooper</b> tar <b>cooper</b> wake huge station  ...  spacefar civil remind <b>cooper</b> amelia brand i there alon <b>cooper</b> tar take spacecraft '],\n",
       " [4704,\n",
       "  'Event Horizon',\n",
       "  '',\n",
       "  'what happen captain <b>miller</b> lewi clark—along secondincommand  ...  dj rescu technician <b>cooper</b> ar also join  ...  correspond fear regret <b>miller</b> see corrick subordin  ...  save yourself hell <b>miller</b> dj deduc ship  ...  back through portal <b>miller</b> decid destroi event  ...  kill smith blast <b>cooper</b> off space weir  ...  corner starck bridg <b>miller</b> confront weir overpow  ...  return other dimens <b>cooper</b> us hi space  ...  space ensu decompress <b>miller</b> starck <b>cooper</b> surviv manag seal  ...  own ship destroi <b>miller</b> plan split event  ...  corrick resurrect weir <b>miller</b> fight off deton  ...  black hole starck <b>cooper</b> enter stasi besid  ...  scream i comfort <b>cooper</b> <b>cooper</b> restrain terrifi starck '],\n",
       " [1954,\n",
       "  'High Noon',\n",
       "  '',\n",
       "  'territori marshal kane <b>cooper</b> newli marri ami  ...  word arriv frank <b>miller</b> ian macdonald viciou  ...  arriv noon train <b>miller</b> gang—hi young brother  ...  station i clear <b>miller</b> intend exact reveng  ...  simple—leav town befor <b>miller</b> arriv kane sens  ...  befor besid sai <b>miller</b> hi gang hunt  ...  jurado wa onc <b>miller</b> lover kane i  ...  entir other ar <b>miller</b> friend resent kane  ...  otto kruger sentenc <b>miller</b> flee horseback urg  ...  goe street face <b>miller</b> hi gang alon  ...  gun down ben <b>miller</b> colbi i wind  ...  belief pick ben <b>miller</b> gun shoot pierc  ...  leav onli frank <b>miller</b> grab ami shield  ...  open ami claw <b>miller</b> face push grind  ...  clear shoot shoot <b>miller</b> dead kane help ']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"\", \"nasa cooper Gargantua Miller\", 10, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBig369C6wSC"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>ارزیابی عملکرد سامانه (۱۰ نمره)</b>\n",
    "    </h1>\n",
    "    همانطور که در بخش مربوط به مجموعه دادگان گفته شد، تعدادی پرسمان نمونه به همراه اسناد مورد نظر برای آن‌ها در اختیار شما قرار گرفته است. در هر مورد اطلاعات لازم برای ایجاد یک پرسمان ذکر شده است. مطابق آن‌ها پرسمان خود را ایجاد کنید. نتایج به دست آمده از هر پرسمان را به عنوان predicted results آن پرسمان در نظر بگیرید. همچنین در هر مورد لیستی از شناسه‌ها وجود دارد. این لیست را به عنوان actual results در نظر بگیرید. <br>\n",
    "    معیار‌های زیر را پیاده‌سازی کنید (بدون استفاده از کد آماده) و نتیجه این معیارها را روی مجموعه‌ی actual و predicted گزارش کنید. دقت کنید که به ازای هر پرسمان باید تمام معیارها را در قالب یک جدول گزارش کنید.<br> دقت کنید که ۳ پرسمان آخر فقط برای افرادی که بخش spell correction را پیاده کرد‌ه‌اند نتیجه مطلوب دارد.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPfEVezQIsmb",
    "outputId": "28b08705-5d4f-47e6-baec-3372f3e3293c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision(actual, predicted):\n",
    "    relevant_retrieved = 0\n",
    "    total_retrieved = 0\n",
    "\n",
    "    for doc_id in predicted:\n",
    "        total_retrieved += 1\n",
    "\n",
    "        if actual.__contains__(doc_id):\n",
    "            relevant_retrieved += 1\n",
    "\n",
    "    return relevant_retrieved / total_retrieved\n",
    "\n",
    "\n",
    "precision([1,2], [1, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "is4nfSw1LQoQ",
    "outputId": "cc2a4388-464b-4d67-cb86-32617e3e526d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recall(actual, predicted):\n",
    "    relevant_retrieved = 0\n",
    "    relevant_exist = len(actual)\n",
    "\n",
    "    for doc_id in predicted:\n",
    "        if actual.__contains__(doc_id):\n",
    "            relevant_retrieved += 1\n",
    "\n",
    "    return relevant_retrieved / relevant_exist\n",
    "\n",
    "\n",
    "recall([1, 2], [1, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYHYwRgTLVRA",
    "outputId": "d749daf0-102d-4855-e292-21dcd137d6be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1_score(actual, predicted):\n",
    "    precision_score = precision(actual, predicted)\n",
    "    recall_score = recall(actual, predicted)\n",
    "\n",
    "    return 2 * (precision_score * recall_score) / (precision_score + recall_score)\n",
    "\n",
    "\n",
    "f1_score([1, 2], [1, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bZg-_PUPLZ31",
    "outputId": "e85b9efa-3b9b-470e-8b93-f474585da6c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_score(actual, predicted):\n",
    "    relevant_retrieved = 0\n",
    "    total_retrieved = 0\n",
    "\n",
    "    precisions = []\n",
    "\n",
    "    for doc_id in predicted:\n",
    "        total_retrieved += 1\n",
    "\n",
    "        if actual.__contains__(doc_id):\n",
    "            relevant_retrieved += 1\n",
    "            precisions.append(relevant_retrieved / total_retrieved)\n",
    "\n",
    "    return sum(precisions) / len(precisions)\n",
    "\n",
    "\n",
    "map_score([1, 2], [1, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OgOKEbMNLf-r",
    "outputId": "b38f683a-7224-411b-efb4-8fe7ce4eb328"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ndcg_score(actual, predicted):\n",
    "    relevant_retrieved = 0\n",
    "    dcg = 0\n",
    "\n",
    "    for i in range(0, len(predicted)):\n",
    "        if actual.__contains__(predicted[i]):\n",
    "            relevant_retrieved += 1\n",
    "            dcg += 1 if i == 0 else 1 / log2(i + 1)\n",
    "\n",
    "    gt_dcg = 0\n",
    "    \n",
    "    for i in range(0, relevant_retrieved):\n",
    "        gt_dcg += 1 if i == 0 else 1 / log2(i + 1)\n",
    "\n",
    "    return dcg / gt_dcg\n",
    "\n",
    "\n",
    "ndcg_score([1, 2], [1, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 : \n",
      "precision:  0.9\n",
      "recall:  0.9\n",
      "f1_score:  0.9\n",
      "map_score:  1.0\n",
      "ndcg_score:  1.0\n",
      "\n",
      "2 : \n",
      "precision:  1.0\n",
      "recall:  1.0\n",
      "f1_score:  1.0\n",
      "map_score:  1.0\n",
      "ndcg_score:  1.0\n",
      "\n",
      "3 : \n",
      "precision:  1.0\n",
      "recall:  1.0\n",
      "f1_score:  1.0\n",
      "map_score:  1.0\n",
      "ndcg_score:  1.0\n",
      "\n",
      "4 : \n",
      "precision:  0.8333333333333334\n",
      "recall:  0.8333333333333334\n",
      "f1_score:  0.8333333333333334\n",
      "map_score:  0.8362626262626263\n",
      "ndcg_score:  0.9202341599612328\n",
      "\n",
      "5 : \n",
      "precision:  0.0\n",
      "recall:  0.0\n",
      "\n",
      "6 : \n",
      "precision:  1.0\n",
      "recall:  1.0\n",
      "f1_score:  1.0\n",
      "map_score:  1.0\n",
      "ndcg_score:  1.0\n",
      "\n",
      "7 : \n",
      "precision:  1.0\n",
      "recall:  1.0\n",
      "f1_score:  1.0\n",
      "map_score:  1.0\n",
      "ndcg_score:  1.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from math import log\n",
    "\n",
    "log2 = lambda x: log(x, 2)\n",
    "\n",
    "\n",
    "with open('validation.json') as json_file:\n",
    "    querys = json.load(json_file)\n",
    "    num = 0\n",
    "\n",
    "    for query in querys:\n",
    "        num += 1\n",
    "        doc_ids = []\n",
    "        max_size = int(query[\"max_size\"])\n",
    "        title_weight = int(query[\"title_weight\"])\n",
    "        plot_query = query[\"plot_query\"]\n",
    "        title_query = query[\"title_query\"]\n",
    "\n",
    "        for doc_id in query[\"doc_ids\"]:\n",
    "            doc_ids.append(int(doc_id))\n",
    "\n",
    "        predicted = [doc[0] for doc in search(title_query, plot_query, title_weight, max_size)]\n",
    "        \n",
    "        try:\n",
    "            print()\n",
    "            print(num, \": \")\n",
    "            print(\"precision: \", precision(doc_ids, predicted))\n",
    "            print(\"recall: \", recall(doc_ids, predicted))\n",
    "            print(\"f1_score: \", f1_score(doc_ids, predicted))\n",
    "            print(\"map_score: \", map_score(doc_ids, predicted))\n",
    "            print(\"ndcg_score: \", ndcg_score(doc_ids, predicted))\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHVlw0kVBUW4"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>نکات پایانی</b>\n",
    "    </h1>\n",
    "    \n",
    "\n",
    "1.   سیستم را بهینه پیاده‌سازی کنید تا در زمان کمتری بارگذاری و نمایه‌سازی  انجام شود.\n",
    "2.   تمام قطعه‌ کدهای بالا باید توسط شما تکمیل شود. نوت‌بوک نهایی باید بدون خطا اجرا شود. اگر تمام کدهای استفاده شده در همین فایل نیست،‌ حتما آن‌ها را نیز به همراه نوت‌بوک در کوئرا آپلود کنید.\n",
    "3.    اسم توابع و نحوه ورودی گرفتن و خروجی دادن آن‌ها را تغییر ندهید. بقیه اجزای توابع صرفا برای شهود بیشتر شما نوشته شده‌اند و نیازی به نگه‌داری آن‌ها نیست.\n",
    "4.   در صورت امکان استفاده از کد آماده مشخصا این مورد برای بخش مربوطه ذکر شده است. اگر چیزی در این مورد ذکر نشده نمی‌توانید از کد آماده استفاده کنید.\n",
    "5.    فایل داده‌ها را در کوئرا آپلود نکنید.\n",
    "6.    فایل زیپ نهایی و فایل نوت‌بوک حتما به فرمت StudentNumber_phase1 نامگذاری شود.\n",
    "\n",
    "\n",
    "<b>سالم و موفق باشید.</b>\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MIR_Phase1_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
